{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "from pedrec.configs.dataset_configs import get_sim_dataset_cfg_default\n",
    "from pedrec.configs.pedrec_net_config import PedRecNet50Config\n",
    "from pedrec.models.constants.dataset_constants import DatasetType\n",
    "from pedrec.visualizers.skeleton_visualizer import draw_skeleton\n",
    "from pedrec.datasets.pedrec_temporal_dataset import PedRecTemporalDataset\n",
    "from pedrec.models.constants.action_mappings import ACTION\n",
    "from pedrec.configs.dataset_configs import get_sim_dataset_cfg_default, PedRecTemporalDatasetConfig\n",
    "from pedrec.configs.pedrec_net_config import PedRecNet50Config\n",
    "from pedrec.models.constants.dataset_constants import DatasetType\n",
    "from pedrec.visualizers.skeleton_visualizer import draw_skeleton\n",
    "from pedrec.datasets.pedrec_temporal_dataset import PedRecTemporalDataset\n",
    "from pedrec.models.constants.action_mappings import ACTION\n",
    "from pedrec.models.constants.sample_method import SAMPLE_METHOD\n",
    "from pedrec.utils.augmentation_helper import get_affine_transform, get_affine_transforms\n",
    "from pedrec.configs.app_config import AppConfig, action_list_c01\n",
    "from pedrec.models.data_structures import ImageSize\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataset_dir = \"data/datasets/Conti01/\"\n",
    "dataset_file_train = \"rt_conti_01_train_FIN.pkl\"\n",
    "dataset_file = \"rt_conti_01_val_FIN.pkl\"\n",
    "dataset_result_file = \"C01F_pred_df_experiment_pedrec_p2d3d_c_o_h36m_sim_mebow_0_allframes.pkl\"\n",
    "model_input_size = ImageSize(width=192, height=256)\n",
    "cfg = PedRecNet50Config()\n",
    "app_cfg = AppConfig()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "dataset_cfg = PedRecTemporalDatasetConfig(\n",
    "    flip=True,\n",
    "    scale_factor=0.25,\n",
    "    rotation_factor=0,\n",
    "    skeleton_3d_range=3000,\n",
    "    img_pattern=\"view_{cam_name}-frame_{id}.{type}\",\n",
    "    subsample=1,\n",
    "    subsampling_strategy=SAMPLE_METHOD.SYSTEMATIC,\n",
    "    gt_result_ratio=1,\n",
    "    use_unit_skeleton=True,\n",
    "    min_joint_score=0,\n",
    "    add_2d=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset = PedRecTemporalDataset(dataset_dir,\n",
    "                                  dataset_file,\n",
    "                                  DatasetType.VALIDATE,\n",
    "                                  dataset_cfg,\n",
    "                                  app_cfg.inference.action_list,\n",
    "                                  None,\n",
    "                                  pose_results_file=dataset_result_file)\n",
    "dataset_length = len(dataset)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ehpi_3d, action_id, annotations = dataset[50]\n",
    "img_path = annotations.img_path\n",
    "img_path = os.path.join(dataset_dir, img_path)\n",
    "img_o = cv2.imread(img_path)\n",
    "trans, trans_inv = get_affine_transforms(annotations.center, annotations.scale, 0, model_input_size, add_inv=True)\n",
    "img_o = cv2.warpAffine(\n",
    "    img_o,\n",
    "    trans,\n",
    "    (int(model_input_size.width), int(model_input_size.height)),\n",
    "    flags=cv2.INTER_LINEAR)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img_o)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(6,3, figsize=(15,15)) \n",
    "\n",
    "count = 0\n",
    "for i in range(0, 3):\n",
    "    for j in range(0, 3):\n",
    "        # TODO: Dazu noch das Bild\n",
    "        entry = dataset[randint(0, dataset_length)]\n",
    "        # entry = dataset[count + 606]\n",
    "        ehpi_3d, action_id, annotations = entry\n",
    "        img_path = annotations.img_path\n",
    "        img_path = os.path.join(dataset_dir, img_path)\n",
    "        img_o = cv2.imread(img_path)\n",
    "        trans, trans_inv = get_affine_transforms(annotations.center, annotations.scale, 0, model_input_size, add_inv=True)\n",
    "        img_o = cv2.warpAffine(\n",
    "            img_o,\n",
    "            trans,\n",
    "            (int(model_input_size.width), int(model_input_size.height)),\n",
    "            flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        actions = \"\"\n",
    "        for idx, is_active in enumerate(action_id):\n",
    "            if is_active == 1:\n",
    "                actions = f\"{actions}{app_cfg.inference.action_list[idx].name}, \"\n",
    "        img = ehpi_3d\n",
    "        ax[i*2, j].imshow(img_o)\n",
    "        ax[i*2+1, j].imshow(img)\n",
    "        ax[i*2, j].set_title(actions)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "fig, ax = plt.subplots(8,4, figsize=(15,15)) \n",
    "action_examples = []\n",
    "count = 0\n",
    "i = 0\n",
    "j = 0\n",
    "while True:\n",
    "    if len(action_examples) == len(action_list_c01):\n",
    "        break\n",
    "    if count >= len(dataset):\n",
    "        print(\"missing a action\")\n",
    "        break\n",
    "    entry = dataset[randint(0, dataset_length)]\n",
    "    # entry = dataset[count + 606]\n",
    "    ehpi_3d, action_ids, annotations = entry\n",
    "    if np.sum(action_ids) > 1:\n",
    "        count += 1\n",
    "        continue\n",
    "    action = None\n",
    "    for idx, is_active in enumerate(action_ids):\n",
    "        if is_active == 1:\n",
    "            action = app_cfg.inference.action_list[idx].name\n",
    "            break\n",
    "    assert action is not None\n",
    "    if action in action_examples:\n",
    "        count += 1\n",
    "        continue\n",
    "    action_examples.append(action)\n",
    "    img_path = annotations.img_path\n",
    "    img_path = os.path.join(dataset_dir, img_path)\n",
    "    img_o = cv2.imread(img_path)\n",
    "    trans, trans_inv = get_affine_transforms(annotations.center, annotations.scale, 0, model_input_size, add_inv=True)\n",
    "    img_o = cv2.warpAffine(\n",
    "        img_o,\n",
    "        trans,\n",
    "        (int(model_input_size.width), int(model_input_size.height)),\n",
    "        flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    ax[i*2, j].imshow(img_o)\n",
    "    ax[i*2+1, j].imshow(ehpi_3d)\n",
    "    ax[i*2, j].set_title(action)\n",
    "    count += 1\n",
    "    i += 1\n",
    "    if i > 3:\n",
    "        j += 1\n",
    "        i = 0\n",
    "    ehpi = cv2.resize(ehpi_3d, (320, 320), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(f\"data/models/ehpi3d/examples/{action}_bb.jpg\", img_o)\n",
    "    cv2.imwrite(f\"data/models/ehpi3d/examples/{action}_ehpi3d.png\", ehpi)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ba19fc6348e4e5cac12775a5de62040"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dennis/code/python/pedrec/notebooks/../pedrec/utils/skeleton_helper_3d.py:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_direction = direction / np.linalg.norm(direction, axis=1, keepdims=True)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aed721f7db71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"missing a action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# entry = dataset[count + 606]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mehpi_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/python/pedrec/notebooks/../pedrec/datasets/pedrec_temporal_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, dataset_idx)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_mappings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mcurr_frame_annotations_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mcurr_frame_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_frame_annotations_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_df_path = os.path.join(dataset_dir, dataset_file)\n",
    "df = pd.read_pickle(dataset_df_path)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.float_format= '{:.2f}'.format\n",
    "print(f\"Number of entrys: {df.shape[0]}\")"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_df_result_path = os.path.join(dataset_dir, dataset_result_file)\n",
    "df_result = pd.read_pickle(dataset_df_result_path)\n",
    "print(f\"Number of entrys: {df_result.shape[0]}\")"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filter_skeleton2d = [col for col in df if col.startswith('skeleton2d')]\n",
    "filter_skeleton3d = [col for col in df if col.startswith('skeleton3d')]\n",
    "filter_bb = [col for col in df if col.startswith('bb')]\n",
    "filter_body_orientation = [col for col in df if col.startswith('body_orientation')]\n",
    "filter_head_orientation = [col for col in df if col.startswith('head_orientation')]\n",
    "filter_env = [col for col in df if col.startswith('env')]"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# skeleton 2d calculated columns\n",
    "skeleton2d_xs = [col for col in df if col.startswith('skeleton2d') and col.endswith('_x')]\n",
    "skeleton2d_ys = [col for col in df if col.startswith('skeleton2d') and col.endswith('_y')]\n",
    "df[\"skeleton2d_width\"] = df[skeleton2d_xs].max(axis=1) - df[skeleton2d_xs].min(axis=1)\n",
    "df[\"skeleton2d_height\"] = df[skeleton2d_ys].max(axis=1) - df[skeleton2d_ys].min(axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# skeleton 3d calculated columns\n",
    "skeleton3d_xs = [col for col in df if col.startswith('skeleton3d') and col.endswith('_x')]\n",
    "skeleton3d_ys = [col for col in df if col.startswith('skeleton3d') and col.endswith('_y')]\n",
    "skeleton3d_zs = [col for col in df if col.startswith('skeleton3d') and col.endswith('_z')]\n",
    "df[\"skeleton3d_width\"] = df[skeleton3d_xs].max(axis=1) - df[skeleton3d_xs].min(axis=1)\n",
    "df[\"skeleton3d_height\"] = df[skeleton3d_ys].max(axis=1) - df[skeleton3d_ys].min(axis=1)\n",
    "df[\"skeleton3d_depth\"] = df[skeleton3d_zs].max(axis=1) - df[skeleton3d_zs].min(axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head(5)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_file = \"rt_conti_01_train.pkl\"  # report only frames with skeletons inside\n",
    "df_val = pd.read_pickle(os.path.join(dataset_dir, dataset_file))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "action_counter = {}\n",
    "for action_, movement_ in zip(df['actions'].iteritems(), df['movement'].iteritems()):\n",
    "    actions = action_[1]\n",
    "    movement = movement_[1]\n",
    "    for action_id in actions:\n",
    "        the_action = action_id\n",
    "        if action_id == ACTION.WALK.value:\n",
    "            if movement == ACTION.JOG.value:\n",
    "                the_action = ACTION.JOG.value\n",
    "        if the_action not in action_counter:\n",
    "            action_counter[the_action] = [0, 0]\n",
    "        action_counter[the_action][0] += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_val = pd.read_pickle(os.path.join(dataset_dir, dataset_file.replace(\"train\", \"val\")))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for action_, movement_ in zip(df_val['actions'].iteritems(), df_val['movement'].iteritems()):\n",
    "    actions = action_[1]\n",
    "    movement = movement_[1]\n",
    "    for action_id in actions:\n",
    "        the_action = action_id\n",
    "        if action_id == ACTION.WALK.value:\n",
    "            if movement == ACTION.JOG.value:\n",
    "                the_action = ACTION.JOG.value\n",
    "        if the_action not in action_counter:\n",
    "            action_counter[the_action] = [0, 0]\n",
    "        action_counter[the_action][1] += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(sorted(action_counter.keys()))\n",
    "for action_id, action_counts in sorted(action_counter.items()):\n",
    "    print(f\"{ACTION(action_id).name.replace('_', ' ')} & {action_counts[0]} & {action_counts[1]} \\\\\\\\\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def save_example(action):\n",
    "  dataset = PedRecTemporalDataset(dataset_dir,\n",
    "                                  dataset_file,\n",
    "                                  DatasetType.VALIDATE,\n",
    "                                  dataset_cfg,\n",
    "                                  app_cfg.inference.action_list,\n",
    "                                  None,\n",
    "                                  action_filters=action.value,\n",
    "                                  pose_results_file=None)\n",
    "  dataset_length = len(dataset)\n",
    "  entry = dataset[400]\n",
    "  ehpi_3d, action_ids, annotations = entry\n",
    "  action = \"\"\n",
    "  for idx, is_active in enumerate(action_ids):\n",
    "      if is_active == 1:\n",
    "          action = f\"{action}{app_cfg.inference.action_list[idx].name}_\"\n",
    "  assert action is not None\n",
    "  action = action[:-1]\n",
    "  img_path = annotations.img_path\n",
    "  img_path = os.path.join(dataset_dir, img_path)\n",
    "  img_o = cv2.imread(img_path)\n",
    "  trans, trans_inv = get_affine_transforms(annotations.center, annotations.scale, 0, model_input_size, add_inv=True)\n",
    "  img_o = cv2.warpAffine(\n",
    "      img_o,\n",
    "      trans,\n",
    "      (int(model_input_size.width), int(model_input_size.height)),\n",
    "      flags=cv2.INTER_LINEAR)\n",
    "\n",
    "  ehpi = cv2.resize(ehpi_3d, (320, 320), interpolation=cv2.INTER_NEAREST)\n",
    "  # print(ehpi_3d)\n",
    "  print(action_ids)\n",
    "  cv2.imwrite(f\"data/models/ehpi3d/examples/{action}_bb.jpg\", img_o)\n",
    "  cv2.imwrite(f\"data/models/ehpi3d/examples/{action}_ehpi3d.png\", ehpi)\n",
    "# save_example(ACTION.FIGHT)\n",
    "# save_example(ACTION.THROW)\n",
    "# save_example(ACTION.KICK_BALL)\n",
    "# save_example(ACTION.ARGUE)\n",
    "# save_example(ACTION.OPEN_DOOR)\n",
    "# save_example(ACTION.WALK)\n",
    "# save_example(ACTION.WAVE)\n",
    "save_example(ACTION.STAND)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "val_dataset = PedRecTemporalDataset(dataset_dir,\n",
    "                                  dataset_file,\n",
    "                                  DatasetType.VALIDATE,\n",
    "                                  dataset_cfg,\n",
    "                                  app_cfg.inference.action_list,\n",
    "                                  None)\n",
    "train_dataset = PedRecTemporalDataset(dataset_dir, dataset_file_train, DatasetType.VALIDATE, dataset_cfg, app_cfg.inference.action_list, None)\n",
    "# dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "# dataset_length = len(dataset)\n",
    "# dataset = val_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "action_counter_val = np.array([0] * len(app_cfg.inference.action_list), dtype=np.float32)\n",
    "for _, label in val_dataset:\n",
    "  action_counter_val += label"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dennis/code/python/pedrec/notebooks/../pedrec/utils/skeleton_helper_3d.py:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_direction = direction / np.linalg.norm(direction, axis=1, keepdims=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "action_counter_train = np.array([0] * len(app_cfg.inference.action_list), dtype=np.float32)\n",
    "for _, label in train_dataset:\n",
    "  action_counter_train += label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "for action_key, counts in enumerate(zip(action_counter_train, action_counter_val)):\n",
    "  count_train, count_val = counts\n",
    "  print(f\"{app_cfg.inference.action_list[action_key].name} & ${int(count_train)}$ & ${int(count_val)}$ \\\\\\\\\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STAND & $409050$ & $45652$ \\\\\n",
      "IDLE & $58206$ & $10168$ \\\\\n",
      "WALK & $501943$ & $57699$ \\\\\n",
      "JOG & $235386$ & $16207$ \\\\\n",
      "WAVE & $47666$ & $2970$ \\\\\n",
      "KICK_BALL & $9451$ & $1144$ \\\\\n",
      "THROW & $8351$ & $1024$ \\\\\n",
      "LOOK_FOR_TRAFFIC & $131570$ & $14819$ \\\\\n",
      "HITCHHIKE & $38288$ & $4018$ \\\\\n",
      "TURN_AROUND & $37380$ & $3494$ \\\\\n",
      "WORK & $34580$ & $4263$ \\\\\n",
      "ARGUE & $13055$ & $961$ \\\\\n",
      "STUMBLE & $4967$ & $837$ \\\\\n",
      "OPEN_DOOR & $13342$ & $1192$ \\\\\n",
      "FALL & $8071$ & $591$ \\\\\n",
      "STAND_UP & $6713$ & $873$ \\\\\n",
      "FIGHT & $15512$ & $990$ \\\\\n",
      "SIT & $0$ & $0$ \\\\\n",
      "JUMP & $0$ & $0$ \\\\\n",
      "WAVE_CAR_OUT & $0$ & $0$ \\\\\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pedrec': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "bf491277edac76033ce6ce8ea4226b9b4057f0df010a0d274c01dafd8b4e2caa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}